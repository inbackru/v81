#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö InBack –∏–∑ Excel –±—ç–∫–∞–ø–æ–≤
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥—É–±–ª–∏ –∏ –æ—à–∏–±–∫–∏ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
"""

import os
import sys
import pandas as pd
from datetime import datetime
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('restore_improved.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ImprovedBackupRestorer:
    def __init__(self):
        self.db_url = os.environ.get('DATABASE_URL')
        if not self.db_url:
            raise ValueError("DATABASE_URL –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
        
        self.engine = create_engine(self.db_url)
        self.Session = sessionmaker(bind=self.engine)
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'total_records': 0,
            'restored_records': 0,
            'updated_records': 0,
            'errors': 0,
            'skipped': 0
        }
        
    def get_latest_file(self, pattern):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π —Ñ–∞–π–ª –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        files = []
        for file in os.listdir('attached_assets'):
            if file.startswith(pattern) and file.endswith('.xlsx'):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º timestamp –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                try:
                    timestamp = file.split('_')[-1].replace('.xlsx', '')
                    files.append((int(timestamp), file))
                except:
                    files.append((0, file))
        
        if not files:
            logger.warning(f"–§–∞–π–ª—ã —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º '{pattern}' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return None
            
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ timestamp –∏ –±–µ—Ä–µ–º —Å–∞–º—ã–π –Ω–æ–≤—ã–π
        files.sort(reverse=True)
        latest_file = files[0][1]
        logger.info(f"–í—ã–±—Ä–∞–Ω —Ñ–∞–π–ª: {latest_file}")
        return latest_file

    def safe_clear_table(self, table_name):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É (—Ç–æ–ª—å–∫–æ DELETE, –±–µ–∑ —Å–±—Ä–æ—Å–∞ constraints)"""
        try:
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                count = result.fetchone()[0]
                
                if count > 0:
                    conn.execute(text(f"DELETE FROM {table_name}"))
                    conn.commit()
                    logger.info(f"–£–¥–∞–ª–µ–Ω–æ {count} –∑–∞–ø–∏—Å–µ–π –∏–∑ —Ç–∞–±–ª–∏—Ü—ã {table_name}")
                else:
                    logger.info(f"–¢–∞–±–ª–∏—Ü–∞ {table_name} —É–∂–µ –ø—É—Å—Ç–∞")
                    
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É {table_name}: {e}")

    def upsert_record(self, conn, table_name, data, unique_columns=['id']):
        """–í—Å—Ç–∞–≤–∏—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å (UPSERT)"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∑–∞–ø–∏—Å—å
            conditions = []
            check_params = {}
            
            for col in unique_columns:
                if col in data:
                    conditions.append(f"{col} = :{col}_check")
                    check_params[f"{col}_check"] = data[col]
            
            if not conditions:
                # –ù–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, –ø—Ä–æ—Å—Ç–æ –≤—Å—Ç–∞–≤–ª—è–µ–º
                cols = list(data.keys())
                insert_sql = text(f"""
                    INSERT INTO {table_name} ({', '.join(cols)})
                    VALUES ({', '.join([f':{col}' for col in cols])})
                """)
                conn.execute(insert_sql, data)
                return 'inserted'
            
            check_sql = text(f"SELECT id FROM {table_name} WHERE {' AND '.join(conditions)}")
            existing = conn.execute(check_sql, check_params).fetchone()
            
            if existing:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
                set_clauses = []
                update_params = check_params.copy()
                
                for col, value in data.items():
                    if col not in unique_columns:
                        set_clauses.append(f"{col} = :{col}_update")
                        update_params[f"{col}_update"] = value
                
                if set_clauses:
                    update_sql = text(f"""
                        UPDATE {table_name} 
                        SET {', '.join(set_clauses)}
                        WHERE {' AND '.join(conditions)}
                    """)
                    conn.execute(update_sql, update_params)
                    return 'updated'
                else:
                    return 'skipped'
            else:
                # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
                cols = list(data.keys())
                insert_sql = text(f"""
                    INSERT INTO {table_name} ({', '.join(cols)})
                    VALUES ({', '.join([f':{col}' for col in cols])})
                """)
                conn.execute(insert_sql, data)
                return 'inserted'
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ UPSERT –≤ {table_name}: {e}")
            raise

    def parse_date_string(self, date_str):
        """–ü–∞—Ä—Å–∏—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç"""
        if not date_str or pd.isna(date_str):
            return None
            
        if not isinstance(date_str, str):
            return date_str
            
        if 'GMT' in date_str:
            try:
                # –§–æ—Ä–º–∞—Ç: 'Wed Aug 27 2025 10:37:41 GMT+0300 (–ú–æ—Å–∫–≤–∞, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –≤—Ä–µ–º—è)'
                date_part = date_str.split(' GMT')[0]
                parsed_date = pd.to_datetime(date_part)
                return parsed_date.strftime('%Y-%m-%d %H:%M:%S')
            except:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É: {date_str}")
                return None
        
        return date_str

    def process_row_data(self, row, df_columns):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏"""
        data = {}
        
        for col in df_columns:
            value = row[col]
            
            if pd.isna(value):
                continue
                
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞—Ç—ã
            if isinstance(value, str) and ('GMT' in value or 'created_at' in col or 'updated_at' in col):
                parsed_date = self.parse_date_string(value)
                if parsed_date:
                    data[col] = parsed_date
            else:
                data[col] = value
                
        return data

    def restore_users(self):
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å UPSERT"""
        logger.info("=== –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ô ===")
        
        file = self.get_latest_file('users')
        if not file:
            return
            
        try:
            df = pd.read_excel(f'attached_assets/{file}')
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(df)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ {file}")
            
            restored = 0
            updated = 0
            
            with self.engine.connect() as conn:
                for _, row in df.iterrows():
                    try:
                        data = self.process_row_data(row, df.columns)
                        if not data:
                            continue
                            
                        result = self.upsert_record(conn, 'users', data, ['id'])
                        
                        if result == 'inserted':
                            restored += 1
                        elif result == 'updated':
                            updated += 1
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {row.get('id', 'unknown')}: {e}")
                        self.stats['errors'] += 1
                
                conn.commit()
                
            logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ - –ù–æ–≤—ã—Ö: {restored}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated}")
            self.stats['restored_records'] += restored
            self.stats['updated_records'] += updated
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {e}")
            self.stats['errors'] += 1

    def restore_managers(self):
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ —Å UPSERT"""
        logger.info("=== –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ú–ï–ù–ï–î–ñ–ï–†–û–í ===")
        
        file = self.get_latest_file('managers')
        if not file:
            return
            
        try:
            df = pd.read_excel(f'attached_assets/{file}')
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(df)} –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –≤ {file}")
            
            restored = 0
            updated = 0
            
            with self.engine.connect() as conn:
                for _, row in df.iterrows():
                    try:
                        data = self.process_row_data(row, df.columns)
                        if not data:
                            continue
                            
                        result = self.upsert_record(conn, 'managers', data, ['id'])
                        
                        if result == 'inserted':
                            restored += 1
                        elif result == 'updated':
                            updated += 1
                        
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ {row.get('id', 'unknown')}: {e}")
                        self.stats['errors'] += 1
                
                conn.commit()
                
            logger.info(f"–ú–µ–Ω–µ–¥–∂–µ—Ä—ã - –ù–æ–≤—ã—Ö: {restored}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated}")
            self.stats['restored_records'] += restored
            self.stats['updated_records'] += updated
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤: {e}")
            self.stats['errors'] += 1

    def restore_excel_properties(self):
        """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å —Å UPSERT"""
        logger.info("=== –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ù–ï–î–í–ò–ñ–ò–ú–û–°–¢–ò ===")
        
        file = self.get_latest_file('excel_properties')
        if not file:
            return
            
        try:
            df = pd.read_excel(f'attached_assets/{file}')
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –≤ {file}")
            
            restored = 0
            updated = 0
            chunk_size = 25  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ—Ä—Ü–∏–∏
            
            with self.engine.connect() as conn:
                for i in range(0, len(df), chunk_size):
                    chunk = df.iloc[i:i + chunk_size]
                    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø–∏—Å–∏ {i+1}-{min(i+chunk_size, len(df))}")
                    
                    for _, row in chunk.iterrows():
                        try:
                            data = self.process_row_data(row, df.columns)
                            if not data or 'inner_id' not in data:
                                continue
                                
                            result = self.upsert_record(conn, 'excel_properties', data, ['inner_id'])
                            
                            if result == 'inserted':
                                restored += 1
                            elif result == 'updated':
                                updated += 1
                            
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ {row.get('inner_id', 'unknown')}: {e}")
                            self.stats['errors'] += 1
                    
                    conn.commit()  # –ö–æ–º–º–∏—Ç–∏–º –∫–∞–∂–¥—É—é –ø–æ—Ä—Ü–∏—é
                
            logger.info(f"–ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å - –ù–æ–≤—ã—Ö: {restored}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated}")
            self.stats['restored_records'] += restored
            self.stats['updated_records'] += updated
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏: {e}")
            self.stats['errors'] += 1

    def restore_generic_table(self, table_pattern, table_name, unique_cols=['id']):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã"""
        logger.info(f"=== –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï {table_name.upper()} ===")
        
        file = self.get_latest_file(table_pattern)
        if not file:
            return
            
        try:
            df = pd.read_excel(f'attached_assets/{file}')
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–∞–±–ª–∏—Ü–µ {table_name}")
            
            if len(df) == 0:
                logger.info(f"–¢–∞–±–ª–∏—Ü–∞ {table_name} –ø—É—Å—Ç–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                return
            
            restored = 0
            updated = 0
            
            with self.engine.connect() as conn:
                for _, row in df.iterrows():
                    try:
                        data = self.process_row_data(row, df.columns)
                        if not data:
                            continue
                            
                        result = self.upsert_record(conn, table_name, data, unique_cols)
                        
                        if result == 'inserted':
                            restored += 1
                        elif result == 'updated':
                            updated += 1
                        elif result == 'skipped':
                            self.stats['skipped'] += 1
                            
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏ –≤ {table_name}: {e}")
                        self.stats['errors'] += 1
                
                conn.commit()
                
            logger.info(f"{table_name} - –ù–æ–≤—ã—Ö: {restored}, –û–±–Ω–æ–≤–ª–µ–Ω–æ: {updated}")
            self.stats['restored_records'] += restored
            self.stats['updated_records'] += updated
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")
            self.stats['errors'] += 1

    def check_database_status(self):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            with self.engine.connect() as conn:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
                tables_to_check = ['users', 'managers', 'excel_properties', 'developers', 'districts']
                
                logger.info("=== –°–û–°–¢–û–Ø–ù–ò–ï –ë–î –ü–û–°–õ–ï –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–Ø ===")
                for table in tables_to_check:
                    try:
                        result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
                        count = result.fetchone()[0]
                        logger.info(f"{table}: {count} –∑–∞–ø–∏—Å–µ–π")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {table}: {e}")
                        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ë–î: {e}")

    def run_restore(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å —É–ª—É—á—à–µ–Ω–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ"""
        logger.info("üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –£–õ–£–ß–®–ï–ù–ù–û–ï –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• INBACK")
        logger.info(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {self.db_url[:50]}...")
        
        start_time = datetime.now()
        
        try:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            self.restore_generic_table('districts', 'districts', ['id'])
            self.restore_generic_table('developers', 'developers', ['id']) 
            self.restore_managers()
            self.restore_users()
            self.restore_excel_properties()
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            additional_tables = [
                ('applications', 'applications'),
                ('blog_articles', 'blog_articles'), 
                ('blog_categories', 'blog_categories'),
                ('blog_posts', 'blog_posts'),
                ('buildings', 'buildings'),
                ('callback_requests', 'callback_requests'),
                ('cities', 'cities'),
                ('collections', 'collections'),
                ('favorite_properties', 'favorite_properties'),
                ('favorite_complexes', 'favorite_complexes'),
                ('it_companies', 'it_companies'),
                ('residential_complexes', 'residential_complexes')
            ]
            
            for pattern, table in additional_tables:
                self.restore_generic_table(pattern, table)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self.check_database_status()
            
            # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("=" * 60)
            logger.info("‚úÖ –£–õ–£–ß–®–ï–ù–ù–û–ï –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
            logger.info(f"üïí –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration}")
            logger.info(f"‚úÖ –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π: {self.stats['restored_records']}")
            logger.info(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {self.stats['updated_records']}")
            logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {self.stats['skipped']}")
            logger.info(f"‚ùå –û—à–∏–±–æ–∫: {self.stats['errors']}")
            logger.info("=" * 60)
            
            if self.stats['errors'] > 0:
                logger.warning(f"‚ö†Ô∏è –ü—Ä–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–∏ –≤–æ–∑–Ω–∏–∫–ª–æ {self.stats['errors']} –æ—à–∏–±–æ–∫. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏.")
            else:
                logger.info("üéâ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ –±–µ–∑ –æ—à–∏–±–æ–∫!")
                
        except Exception as e:
            logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
            return False
            
        return True

if __name__ == "__main__":
    try:
        restorer = ImprovedBackupRestorer()
        success = restorer.run_restore()
        
        if success:
            print("\nüéâ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            print("üìã –ü–æ–¥—Ä–æ–±–Ω—ã–π –ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: restore_improved.log")
        else:
            print("\n‚ùå –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∏–ª–æ—Å—å —Å –æ—à–∏–±–∫–∞–º–∏!")
            print("üìã –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥ —Ñ–∞–π–ª: restore_improved.log")
            
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {e}")
        sys.exit(1)