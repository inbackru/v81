#!/usr/bin/env python3
"""
ÐœÐ½Ð¾Ð³Ð¾Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐµÑ€ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ñ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¼ Selenium
"""

import logging
import time
import re
from typing import List, Dict
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

def create_stealth_driver():
    """Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ ÑÐºÑ€Ñ‹Ñ‚Ñ‹Ð¹ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€ Ñ Ð·Ð°Ñ‰Ð¸Ñ‚Ð¾Ð¹ Ð¾Ñ‚ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸"""
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--allow-running-insecure-content')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        driver.set_page_load_timeout(30)
        return driver
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ð°: {e}")
        return None

def scrape_multiple_sources() -> Dict:
    """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²"""
    
    sources = [
        'https://krasnodar.etagi.com/zastr/builders/',
        'https://krasnodar.domclick.ru/zastroishchiki',
        'https://krasnodar.cian.ru/developers/',
        'https://novostroyki.su/krasnodar/developers/',
    ]
    
    all_developers = []
    driver = None
    
    try:
        driver = create_stealth_driver()
        if not driver:
            raise Exception("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€")
        
        for url in sources:
            try:
                logger.info(f"ðŸŒ ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº: {url}")
                
                # ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ
                driver.get(url)
                
                # Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° - ÑÐ¾ÐºÑ€Ð°Ñ‚Ð¸Ð»Ð¸ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ñ
                time.sleep(3)
                
                # Ð‘Ñ‹ÑÑ‚Ñ€Ñ‹Ð¹ ÑÐºÑ€Ð¾Ð»Ð» Ð´Ð»Ñ etagi.com
                if 'etagi.com' in url:
                    logger.info("Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° etagi.com")
                    
                    # ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÐºÑ€Ð¾Ð»Ð» Ð´Ð¾ ÐºÐ¾Ð½Ñ†Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(3)
                    
                    # ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ ÐºÐ»Ð¸ÐºÐ½ÑƒÑ‚ÑŒ ÐºÐ½Ð¾Ð¿ÐºÑƒ "ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÐµÑ‰Ðµ" Ð¾Ð´Ð¸Ð½ Ñ€Ð°Ð·
                    try:
                        show_more_btn = driver.find_element(By.XPATH, "//button[contains(text(), 'ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ') or contains(text(), 'Ð•Ñ‰Ðµ')]")
                        driver.execute_script("arguments[0].click();", show_more_btn)
                        time.sleep(2)
                        logger.info("ÐÐ°Ð¶Ð°Ñ‚Ð° ÐºÐ½Ð¾Ð¿ÐºÐ° 'ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÐµÑ‰Ðµ'")
                    except:
                        pass
                    
                else:
                    # Ð”Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑÐ°Ð¹Ñ‚Ð¾Ð² Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ ÑÐºÑ€Ð¾Ð»Ð»
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(1)
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ HTML
                html = driver.page_source
                logger.info(f"âœ… ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ HTML Ñ€Ð°Ð·Ð¼ÐµÑ€Ð¾Ð¼ {len(html)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ñ {url}")
                
                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð²
                if 'etagi.com' in url:
                    developers = parse_etagi_developers(html, url)
                elif 'domclick.ru' in url:
                    developers = parse_domclick_developers(html, url)
                elif 'cian.ru' in url:
                    developers = parse_cian_developers(html, url)
                else:
                    developers = parse_generic_developers(html, url)
                
                if developers:
                    logger.info(f"ðŸ¢ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(developers)} Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ð½Ð° {url}")
                    all_developers.extend(developers)
                else:
                    logger.warning(f"âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ð½Ð° {url}")
                    
            except Exception as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ðµ {url}: {e}")
                continue
        
        # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ Ð¿Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑŽ
        unique_developers = []
        seen_names = set()
        
        for dev in all_developers:
            name_clean = re.sub(r'\s+', ' ', dev['name'].strip().lower())
            if name_clean not in seen_names:
                seen_names.add(name_clean)
                unique_developers.append(dev)
        
        return {
            'success': len(unique_developers) > 0,
            'developers': unique_developers,
            'total_sources': len(sources),
            'successful_sources': len([d for d in all_developers if d]),
            'unique_developers': len(unique_developers)
        }
        
    except Exception as e:
        logger.error(f"ÐžÐ±Ñ‰Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {e}")
        return {
            'success': False,
            'error': str(e),
            'developers': []
        }
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("ðŸ”» Ð‘Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð·Ð°ÐºÑ€Ñ‹Ñ‚")
            except:
                pass

def parse_etagi_developers(html: str, url: str) -> List[Dict]:
    """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ñ etagi.com"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        # Ð¡Ð¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð»Ñ etagi.com - Ð¸Ñ‰ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð²
        
        # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¸Ñ‰ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ð¿Ð¾ ÑÐ¿ÐµÑ†Ð¸Ñ„Ð¸Ñ‡Ð½Ñ‹Ð¼ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð°Ð¼
        developer_cards = soup.find_all(['div', 'article', 'section'], class_=lambda x: x and any(
            keyword in str(x).lower() for keyword in ['card', 'item', 'developer', 'builder', 'company']
        ))
        
        # Ð¢Ð°ÐºÐ¶Ðµ Ð¸Ñ‰ÐµÐ¼ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð²
        developer_links = soup.find_all('a', href=re.compile(r'/zastr/|/developer|/builder'))
        
        logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(developer_cards)} ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐµÐº Ð¸ {len(developer_links)} ÑÑÑ‹Ð»Ð¾Ðº Ð½Ð° etagi.com")
        
        # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ð¾Ñ‡ÐºÐ¸
        for card in developer_cards:
            try:
                # Ð˜Ñ‰ÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð² Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ñ…
                name_elem = card.find(['h1', 'h2', 'h3', 'h4', 'a', 'span'], string=re.compile(r'[Ð-Ð¯][Ð°-Ñ]{2,}'))
                
                if not name_elem:
                    # Ð˜Ñ‰ÐµÐ¼ Ð¿Ð¾ ÐºÐ»Ð°ÑÑÐ°Ð¼
                    name_elem = card.find(['div', 'span'], class_=lambda x: x and any(
                        keyword in str(x).lower() for keyword in ['title', 'name', 'company']
                    ))
                
                if name_elem:
                    name = name_elem.get_text(strip=True)
                    # Ð¢Ñ‰Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¾Ñ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¾Ñ‚ Ð»Ð¸ÑˆÐ½ÐµÐ³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°
                    name = re.sub(r'ÐžÐ¿Ñ‹Ñ‚ Ð² ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ðµ.*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ "ÐžÐ¿Ñ‹Ñ‚ Ð² ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ðµ..."
                    name = re.sub(r'ÐšÐ²Ð°Ñ€Ñ‚Ð¸Ñ€ Ð² Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ðµ.*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ "ÐšÐ²Ð°Ñ€Ñ‚Ð¸Ñ€ Ð² Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ðµ..."
                    name = re.sub(r'Ð‘Ð¾Ð»ÐµÐµ \d+.*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ "Ð‘Ð¾Ð»ÐµÐµ 123..."
                    name = re.sub(r'\d+ Ð»ÐµÑ‚.*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ "16 Ð»ÐµÑ‚..."
                    name = re.sub(r'\d+ Ð³Ð¾Ð´Ð°.*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ "23 Ð³Ð¾Ð´Ð°..."
                    name = re.sub(r'\s*\(.*?\)\s*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÐºÐ¾Ð±ÐºÐ¸
                    name = re.sub(r'\s*â€”.*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»Ðµ Ñ‚Ð¸Ñ€Ðµ
                    name = re.sub(r'\s*-.*', '', name)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¿Ð¾ÑÐ»Ðµ Ð´ÐµÑ„Ð¸ÑÐ°
                    name = name.strip()
                    
                    if len(name) > 2 and len(name) < 100 and not any(d['name'] == name for d in developers):
                        developers.append({
                            'name': name,
                            'description': f'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº {name} - Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ñ etagi.com',
                            'source': 'etagi.com',
                            'url': url,
                            'specialization': 'Ð–Ð¸Ð»Ð¸Ñ‰Ð½Ð¾Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾'
                        })
                        logger.info(f"  âœ… {name}")
                        
            except Exception as e:
                continue
        
        # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÑÑÑ‹Ð»ÐºÐ¸
        for link in developer_links:
            try:
                name = link.get_text(strip=True)
                if name and len(name) > 2 and len(name) < 100 and not any(d['name'] == name for d in developers):
                    # ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ
                    name = re.sub(r'ÐžÐ¿Ñ‹Ñ‚ Ð² ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ðµ.*', '', name)
                    name = re.sub(r'ÐšÐ²Ð°Ñ€Ñ‚Ð¸Ñ€ Ð² Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ðµ.*', '', name)
                    name = re.sub(r'Ð‘Ð¾Ð»ÐµÐµ \d+.*', '', name)
                    name = re.sub(r'\d+ Ð»ÐµÑ‚.*', '', name)
                    name = re.sub(r'\d+ Ð³Ð¾Ð´Ð°.*', '', name)
                    name = re.sub(r'\s*\(.*?\)\s*', '', name)
                    name = re.sub(r'\s*â€”.*', '', name)
                    name = re.sub(r'\s*-.*', '', name)
                    name = name.strip()
                    if name:
                        developers.append({
                            'name': name,
                            'description': f'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº {name} - Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ñ etagi.com',
                            'source': 'etagi.com',
                            'url': url,
                            'specialization': 'Ð–Ð¸Ð»Ð¸Ñ‰Ð½Ð¾Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾'
                        })
                        logger.info(f"  âœ… {name}")
            except:
                continue
        
        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸Ñ‰ÐµÐ¼ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ
        all_text = soup.get_text()
        
        # Ð Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð²ÑÐµÑ… Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð²
        patterns = [
            # ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹
            r'([Ð-Ð¯][Ð°-Ñ]+(?:\s+[Ð-Ð¯][Ð°-Ñ]+)*)\s*[-â€“â€”]\s*Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº',
            r'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº:\s*([Ð-Ð¯][Ð°-Ñ\s]+)',
            r'([Ð-Ð¯][Ð°-Ñ]{2,}\s+Ð¡Ñ‚Ñ€Ð¾Ð¹[Ð°-Ñ]*)',
            r'([Ð-Ð¯][Ð°-Ñ]{2,}\s+Ð”ÐµÐ²ÐµÐ»Ð¾Ð¿Ð¼ÐµÐ½Ñ‚)',
            r'([Ð-Ð¯][Ð°-Ñ]{2,}\s+Ð˜Ð½Ð²ÐµÑÑ‚[Ð°-Ñ]*)',
            r'([Ð-Ð¯][Ð°-Ñ]{2,}\s+Ð“Ñ€ÑƒÐ¿Ð¿[Ð°-Ñ]*)',
            r'Ð“Ðš\s+["\Â«]?([Ð-Ð¯][Ð°-Ñ\s\-]{3,})["\Â»]?',
            r'ÐžÐžÐž\s+["\Â«]?([Ð-Ð¯][Ð°-Ñ\s\-]{3,})["\Â»]?',
            r'([Ð-Ð¯]{2,}[Ð°-Ñ]*)\s*(?:â€”|â€“|-)\s*(?:ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ|Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº)',
            r'ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ\s+["\Â«]?([Ð-Ð¯][Ð°-Ñ\s\-]{3,})["\Â»]?',
            r'([Ð-Ð¯][Ð°-Ñ]{3,})\s+(?:Ð¥Ð¾Ð»Ð´Ð¸Ð½Ð³|Ð¢Ñ€Ð°ÑÑ‚|ÐšÐ¾Ñ€Ð¿Ð¾Ñ€Ð°Ñ†Ð¸Ñ)',
            # Ð˜Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ðµ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¸ ÐšÑ€Ð°ÑÐ½Ð¾Ð´Ð°Ñ€Ð° (Ð±Ð¾Ð»ÐµÐµ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹)
            r'(Ð¡Ð¡Ðš)',
            r'(Ð¡Ð¿ÐµÑ†Ð¡Ñ‚Ñ€Ð¾Ð¹ÐšÑƒÐ±Ð°Ð½ÑŒ)',
            r'(ÐÐ“Ðš)',
            r'(ÐÐ±ÑÐ¾Ð»ÑŽÑ‚[\s\-]*[Ð“Ð³]Ñ€ÑƒÐ¿Ð¿?)',
            r'(Ð”ÐžÐ“ÐœÐ)',
            r'(Ð”Ð¾Ð³Ð¼Ð°)',
            r'(ÐŸÑ€ÐµÐ¼ÑŒÐµÑ€)',
            r'(Ð¡Ñ‚Ñ€Ð¾Ð¹Ð³Ñ€Ð°Ð´)',
            r'(ÐÐ¾Ð²Ð°Ñ[\s\-]*Ð¡Ð¾Ñ‡Ð¸)',
            r'(Ð®Ð¶Ð½Ñ‹Ð¹[\s\-]*Ð”Ð¾Ð¼)',
            r'(ÐšÑ€Ð°ÑÐ½Ð¾Ð´Ð°Ñ€[\s\-]*Ð¡Ñ‚Ñ€Ð¾Ð¹)',
            r'(ÐžÐ»Ð¸Ð¼Ð¿[\s\-]*Ð¡Ñ‚Ñ€Ð¾Ð¹)',
            r'(ÐœÐ–Ðš)',
            r'(ÐœÐ¾Ð»Ð¾Ð´ÐµÐ¶Ð½Ñ‹Ð¹[\s\-]*Ð¶Ð¸Ð»Ð¸Ñ‰Ð½Ñ‹Ð¹[\s\-]*ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑ)',
            r'(Ð­Ñ‚Ð°Ð¶Ð¸)',
            r'(Ð Ð¾Ð¼ÐµÐºÑ[\s\-]*Ð”ÐµÐ²ÐµÐ»Ð¾Ð¿Ð¼ÐµÐ½Ñ‚)',
            r'(Ð¡Ð¼ÐµÐ½Ð°)',
            r'(ÐÐ»ÑŒÑ„Ð°[\s\-]*Ð¡Ñ‚Ñ€Ð¾Ð¹)',
            r'(Ð’ÐµÐºÑ‚Ð¾Ñ€)',
            r'(Ð“Ð˜Ðš[\s\-]*Ð“Ðš)',
            r'(ÐÐ¡Ðš)',
            r'(Ð‘Ð°ÑƒÐ²ÐµÑÑ‚)',
            r'(Ð˜Ð½Ð³Ñ€Ð°Ð´)',
            r'(Ð¡Ð¸Ð¼Ð¿Ð»ÐµÐºÑ)',
            r'(ÐÐ»ÑŒÑ„Ð°Ð¡Ñ‚Ñ€Ð¾Ð¹Ð˜Ð½Ð²ÐµÑÑ‚)',
            r'(ÐÐ Ð¢[\s\-]*Ð“Ð Ð£ÐŸÐŸ)',
            r'(ÐÐ»ÑŒÐ¿Ð¸ÐºÐ°[\s\-]*Ð“Ñ€ÑƒÐ¿Ð¿)',
            # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¾Ð±Ñ‰Ð¸Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹
            r'([Ð-Ð¯][Ð°-Ñ]{2,}(?:ÑÑ‚Ñ€Ð¾Ð¹|Ð¸Ð½Ð²ÐµÑÑ‚|Ð³Ñ€ÑƒÐ¿Ð¿|Ñ…Ð¾Ð»Ð´Ð¸Ð½Ð³))',
            r'([Ð-Ð¯]{2,}[Ð°-Ñ]*)\s*(?:\(|\[).*(?:ÑÑ‚Ñ€Ð¾Ð¹|Ð´ÐµÐ²ÐµÐ»Ð¾Ð¿Ð¼ÐµÐ½Ñ‚|Ð¸Ð½Ð²ÐµÑÑ‚)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, all_text)
            for match in matches:
                name = match.strip()
                # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° Ð´Ð»Ñ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð²
                name = re.sub(r'ÐžÐ¿Ñ‹Ñ‚ Ð² ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ðµ.*', '', name)
                name = re.sub(r'ÐšÐ²Ð°Ñ€Ñ‚Ð¸Ñ€ Ð² Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ðµ.*', '', name)
                name = re.sub(r'Ð‘Ð¾Ð»ÐµÐµ \d+.*', '', name)
                name = re.sub(r'\d+ Ð»ÐµÑ‚.*', '', name)
                name = re.sub(r'\d+ Ð³Ð¾Ð´Ð°.*', '', name)
                name = name.strip()
                
                if len(name) > 2 and len(name) < 80 and not any(d['name'] == name for d in developers):
                    # Ð˜ÑÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð¾Ð±Ñ‰Ð¸Ðµ ÑÐ»Ð¾Ð²Ð°
                    excluded_words = ['Ð¶Ð¸Ð»Ñ‹Ðµ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÑ‹', 'Ð½Ð¾Ð²Ð¾ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸', 'Ð²ÑÐµ Ð½Ð¾Ð²Ð¾ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸', 'ÐºÐ²Ð°Ñ€Ñ‚Ð¸Ñ€Ñ‹', 'Ð´Ð¾Ð¼Ð°']
                    if name.lower() not in excluded_words:
                        developers.append({
                            'name': name,
                            'description': f'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº {name} - Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ñ etagi.com',
                            'source': 'etagi.com',
                            'url': url,
                            'specialization': 'Ð–Ð¸Ð»Ð¸Ñ‰Ð½Ð¾Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾'
                        })
                        logger.info(f"  âœ… {name}")
        
        return developers
        
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° etagi.com: {e}")
        return []

def parse_domclick_developers(html: str, url: str) -> List[Dict]:
    """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ñ domclick.ru"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        # Ð˜Ñ‰ÐµÐ¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ñ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ°Ð¼Ð¸
        table_rows = soup.find_all('tr')
        
        for row in table_rows:
            try:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 4:
                    name_text = cells[0].get_text(strip=True)
                    if name_text and len(name_text) > 2 and name_text not in ['Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº', 'ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ']:
                        
                        completed_text = cells[1].get_text(strip=True) if len(cells) > 1 else '0'
                        under_construction_text = cells[2].get_text(strip=True) if len(cells) > 2 else '0'
                        on_time_text = cells[3].get_text(strip=True) if len(cells) > 3 else '0%'
                        contact_text = cells[4].get_text(strip=True) if len(cells) > 4 else ''
                        
                        # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ñ‡Ð¸ÑÐ»Ð°
                        completed_match = re.search(r'(\d+)', completed_text)
                        under_construction_match = re.search(r'(\d+)', under_construction_text)
                        on_time_match = re.search(r'(\d+)', on_time_text)
                        
                        completed_buildings = int(completed_match.group(1)) if completed_match else 0
                        under_construction = int(under_construction_match.group(1)) if under_construction_match else 0
                        on_time_percentage = int(on_time_match.group(1)) if on_time_match else 100
                        
                        phone_match = re.search(r'\+7\s?\(?\d{3}\)?\s?\d{3}-?\d{2}-?\d{2}', contact_text)
                        phone = phone_match.group(0) if phone_match else ''
                        
                        developer_data = {
                            'name': name_text,
                            'completed_buildings': completed_buildings,
                            'under_construction': under_construction,
                            'on_time_percentage': on_time_percentage,
                            'phone': phone,
                            'source': 'domclick.ru',
                            'url': url,
                            'specialization': 'Ð–Ð¸Ð»Ð¸Ñ‰Ð½Ð¾Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾',
                            'description': f'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº {name_text} - ÑÐ´Ð°Ð½Ð¾: {completed_buildings}, ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑÑ: {under_construction}, Ð²Ð¾Ð²Ñ€ÐµÐ¼Ñ: {on_time_percentage}%'
                        }
                        
                        developers.append(developer_data)
                        logger.info(f"  âœ… {name_text}")
                        
            except Exception as e:
                continue
        
        # Ð•ÑÐ»Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°, Ð¸Ñ‰ÐµÐ¼ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ñƒ
        if not developers:
            all_text = soup.get_text()
            patterns = [
                r'([Ð-Ð¯][Ð°-Ñ]{2,}\s+Ð¡Ñ‚Ñ€Ð¾Ð¹[Ð°-Ñ]*)',
                r'([Ð-Ð¯][Ð°-Ñ]{2,}\s+Ð”ÐµÐ²ÐµÐ»Ð¾Ð¿Ð¼ÐµÐ½Ñ‚)',
                r'Ð“Ðš\s+([Ð-Ð¯][Ð°-Ñ\s]{3,})',
                r'([Ð-Ð¯][Ð°-Ñ]{2,}\s+Ð“Ñ€ÑƒÐ¿Ð¿[Ð°-Ñ]*)',
                r'(Ð¡Ð¡Ðš|Ð¡Ð¿ÐµÑ†Ð¡Ñ‚Ñ€Ð¾Ð¹ÐšÑƒÐ±Ð°Ð½ÑŒ)',
                r'(ÐÐ“Ðš|ÐÐ±ÑÐ¾Ð»ÑŽÑ‚)',
                r'(Ð”ÐžÐ“ÐœÐ|Ð”Ð¾Ð³Ð¼Ð°)',
                r'(ÐŸÑ€ÐµÐ¼ÑŒÐµÑ€)',
                r'(Ð¡Ñ‚Ñ€Ð¾Ð¹Ð³Ñ€Ð°Ð´)',
                r'(ÐšÑ€Ð°ÑÐ½Ð¾Ð´Ð°Ñ€\s*Ð¡Ñ‚Ñ€Ð¾Ð¹)'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, all_text)
                for match in matches:
                    name = match.strip()
                    if len(name) > 2 and len(name) < 80 and not any(d['name'] == name for d in developers):
                        developers.append({
                            'name': name,
                            'source': 'domclick.ru',
                            'url': url,
                            'specialization': 'Ð–Ð¸Ð»Ð¸Ñ‰Ð½Ð¾Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾',
                            'description': f'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº {name} Ñ domclick.ru'
                        })
        
        return developers
        
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° domclick.ru: {e}")
        return []

def parse_cian_developers(html: str, url: str) -> List[Dict]:
    """ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ñ cian.ru"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        all_text = soup.get_text()
        
        patterns = [
            r'([Ð-Ð¯][Ð°-Ñ]+\s+Ð¡Ñ‚Ñ€Ð¾Ð¹[Ð°-Ñ]*)',
            r'([Ð-Ð¯][Ð°-Ñ]+\s+Ð”ÐµÐ²ÐµÐ»Ð¾Ð¿Ð¼ÐµÐ½Ñ‚)',
            r'([Ð-Ð¯][Ð°-Ñ]+\s+Ð˜Ð½Ð²ÐµÑÑ‚[Ð°-Ñ]*)',
            r'Ð“Ðš\s+([Ð-Ð¯][Ð°-Ñ\s]+)',
            r'([Ð-Ð¯][Ð°-Ñ]+\s+Ð“Ñ€ÑƒÐ¿Ð¿[Ð°-Ñ]*)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, all_text)
            for match in matches:
                name = match.strip()
                if len(name) > 3 and len(name) < 80 and not any(d['name'] == name for d in developers):
                    developers.append({
                        'name': name,
                        'description': f'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº {name} Ñ cian.ru',
                        'source': 'cian.ru',
                        'url': url,
                        'specialization': 'Ð–Ð¸Ð»Ð¸Ñ‰Ð½Ð¾Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾'
                    })
                    logger.info(f"  âœ… {name}")
        
        return developers
        
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° cian.ru: {e}")
        return []

def parse_generic_developers(html: str, url: str) -> List[Dict]:
    """Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ð¸Ð· HTML"""
    try:
        soup = BeautifulSoup(html, 'html.parser')
        developers = []
        
        # Ð˜Ñ‰ÐµÐ¼ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ñ‚ÐµÐºÑÑ‚Ð° Ñ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑÐ¼Ð¸ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¹
        text_content = soup.get_text()
        
        # ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð²
        company_patterns = [
            r'(?:ÐžÐžÐž|ÐžÐÐž|Ð—ÐÐž|ÐÐž|ÐŸÐÐž)\s+["\Â«]?([Ð-Ð¯][Ð°-Ñ\-\s]+)["\Â»]?',
            r'([Ð-Ð¯][Ð°-Ñ]+(?:\s+[Ð-Ð¯][Ð°-Ñ]+)*)\s+(?:Ð¡Ñ‚Ñ€Ð¾Ð¹|Ð”ÐµÐ²ÐµÐ»Ð¾Ð¿Ð¼ÐµÐ½Ñ‚|Ð˜Ð½Ð²ÐµÑÑ‚|Ð“Ñ€ÑƒÐ¿Ð¿|Ð“Ðš|Ð¥Ð¾Ð»Ð´Ð¸Ð½Ð³)',
            r'Ð“Ñ€ÑƒÐ¿Ð¿Ð°\s+ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¹\s+["\Â«]?([Ð-Ð¯][Ð°-Ñ\-\s]+)["\Â»]?',
            r'Ð“Ðš\s+["\Â«]?([Ð-Ð¯][Ð°-Ñ\-\s]+)["\Â»]?',
            r'ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ\s+["\Â«]?([Ð-Ð¯][Ð°-Ñ\-\s]+)["\Â»]?'
        ]
        
        for pattern in company_patterns:
            matches = re.findall(pattern, text_content, re.MULTILINE)
            for match in matches:
                name = match.strip() if isinstance(match, str) else match[0].strip()
                if len(name) > 3 and len(name) < 100 and not any(d['name'] == name for d in developers):
                    developers.append({
                        'name': name,
                        'source': f'generic_{url}',
                        'url': url,
                        'specialization': 'Ð–Ð¸Ð»Ð¸Ñ‰Ð½Ð¾Ðµ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾',
                        'description': f'Ð—Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸Ðº {name} Ð½Ð°Ð¹Ð´ÐµÐ½ Ð½Ð° {url}'
                    })
        
        logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(developers)} Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð² Ñ‡ÐµÑ€ÐµÐ· Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹")
        return developers
        
    except Exception as e:
        logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {e}")
        return []

if __name__ == "__main__":
    # Ð¢ÐµÑÑ‚ Ð¿Ð°Ñ€ÑÐµÑ€Ð°
    print("ðŸ§ª Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼Ð½Ð¾Ð³Ð¾Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²Ð¾Ð³Ð¾ Ð¿Ð°Ñ€ÑÐµÑ€Ð°...")
    
    result = scrape_multiple_sources()
    
    print(f"âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: success={result['success']}")
    if result['success']:
        print(f"ðŸ¢ ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð²: {len(result['developers'])}")
        print(f"ðŸ“Š Ð£ÑÐ¿ÐµÑˆÐ½Ñ‹Ñ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²: {result.get('successful_sources', 0)}")
        print("\nÐŸÐµÑ€Ð²Ñ‹Ðµ 15 Ð·Ð°ÑÑ‚Ñ€Ð¾Ð¹Ñ‰Ð¸ÐºÐ¾Ð²:")
        for i, dev in enumerate(result['developers'][:15], 1):
            print(f"  {i}. {dev['name']} (Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº: {dev['source']})")
    else:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {result.get('error', 'ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°')}")