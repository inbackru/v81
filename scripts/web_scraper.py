import trafilatura
import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
from urllib.parse import urljoin, urlparse
import re
from typing import Dict, List, Optional, Tuple

class KrasnodarDeveloperScraper:
    """
    –ü–∞—Ä—Å–µ—Ä –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ñ–ö –∏ –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∏ –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä–∞
        self.developers = {
            'ssk': {
                'name': '–°–°–ö (–°–ø–µ—Ü–°—Ç—Ä–æ–π–ö—É–±–∞–Ω—å)',
                'website': 'https://sskuban.ru',
                'parser': self.parse_ssk
            },
            'neometria': {
                'name': '–ù–µ–æ–º–µ—Ç—Ä–∏—è',
                'website': 'https://neometria.ru',
                'parser': self.parse_neometria
            },
            'yugstroyinvest': {
                'name': '–Æ–≥–°—Ç—Ä–æ–π–ò–Ω–≤–µ—Å—Ç',
                'website': 'https://yugi.ru',
                'parser': self.parse_yugstroyinvest
            }
        }
    
    def get_website_text_content(self, url: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–∞–π—Ç–∞ —Å –ø–æ–º–æ—â—å—é trafilatura
        """
        try:
            downloaded = trafilatura.fetch_url(url)
            if downloaded:
                text = trafilatura.extract(downloaded)
                return text or ""
            return ""
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å {url}: {e}")
            return ""
    
    def parse_ssk(self) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –°–°–ö - sskuban.ru
        """
        print("–ü–∞—Ä—Å–∏–Ω–≥ –°–°–ö...")
        developer_data = []
        
        try:
            # –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –°–°–ö
            main_url = "https://sskuban.ru"
            response = self.session.get(main_url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –ü–æ–∏—Å–∫ –ñ–ö –Ω–∞ —Å–∞–π—Ç–µ –°–°–ö
                projects_links = soup.find_all('a', href=re.compile(r'/projects/|/zhk/|/complex/'))
                
                for link in projects_links[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–º–∏ 5 –¥–ª—è —Ç–µ—Å—Ç–∞
                    project_url = urljoin(main_url, link.get('href'))
                    project_name = link.get_text(strip=True)
                    
                    if project_name and len(project_name) > 3:
                        project_data = self.parse_project_details(project_url, project_name, '–°–°–ö')
                        if project_data:
                            developer_data.append(project_data)
                            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –°–°–ö: {e}")
        
        return developer_data
    
    def parse_neometria(self) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –ù–µ–æ–º–µ—Ç—Ä–∏—è - neometria.ru
        """
        print("–ü–∞—Ä—Å–∏–Ω–≥ –ù–µ–æ–º–µ—Ç—Ä–∏—è...")
        developer_data = []
        
        try:
            # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤ –ù–µ–æ–º–µ—Ç—Ä–∏–∏
            projects_url = "https://neometria.ru/projects/"
            response = self.session.get(projects_url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # –ü–æ–∏—Å–∫ –ñ–ö
                project_cards = soup.find_all(['div', 'section'], class_=re.compile(r'project|complex|card'))
                
                for card in project_cards[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∞
                    link = card.find('a')
                    if link:
                        project_url = urljoin(projects_url, link.get('href'))
                        project_name = card.get_text(strip=True)[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                        
                        if project_name and '–ñ–ö' in project_name or '–∂–∫' in project_name.lower():
                            project_data = self.parse_project_details(project_url, project_name, '–ù–µ–æ–º–µ—Ç—Ä–∏—è')
                            if project_data:
                                developer_data.append(project_data)
                                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ù–µ–æ–º–µ—Ç—Ä–∏—è: {e}")
        
        return developer_data
    
    def parse_yugstroyinvest(self) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ –Æ–≥–°—Ç—Ä–æ–π–ò–Ω–≤–µ—Å—Ç - –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞
        """
        print("–ü–∞—Ä—Å–∏–Ω–≥ –Æ–≥–°—Ç—Ä–æ–π–ò–Ω–≤–µ—Å—Ç...")
        # –ó–∞–≥–ª—É—à–∫–∞ - –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–∞–π—Ç
        return []
    
    def parse_project_details(self, url: str, name: str, developer: str) -> Optional[Dict]:
        """
        –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö
        """
        try:
            time.sleep(1)  # –ò–∑–±–µ–≥–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            
            response = self.session.get(url, timeout=10)
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            project_info = {
                'name': name,
                'developer': developer,
                'url': url,
                'scraped_at': datetime.now().isoformat(),
                'apartments': []
            }
            
            # –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö
            # –ò—â–µ–º —Ü–µ–Ω—ã
            price_elements = soup.find_all(text=re.compile(r'\d+\s*(?:–º–ª–Ω|—Ç—ã—Å|—Ä—É–±|\‚ÇΩ)'))
            prices = []
            for price_text in price_elements[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                price_match = re.search(r'(\d+(?:\.\d+)?)\s*(?:–º–ª–Ω|—Ç—ã—Å)', str(price_text))
                if price_match:
                    price_val = float(price_match.group(1))
                    if '–º–ª–Ω' in str(price_text):
                        price_val *= 1000000
                    elif '—Ç—ã—Å' in str(price_text):
                        price_val *= 1000
                    if 1000000 <= price_val <= 50000000:  # –†–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã –¥–ª—è –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä–∞
                        prices.append(int(price_val))
            
            # –ü–æ–∏—Å–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
            room_elements = soup.find_all(text=re.compile(r'\d+\s*(?:-–∫|–∫–æ–º–Ω|–∫\.|room)'))
            rooms = []
            for room_text in room_elements[:10]:
                room_match = re.search(r'(\d+)\s*(?:-–∫|–∫–æ–º–Ω|–∫\.)', str(room_text))
                if room_match:
                    room_count = int(room_match.group(1))
                    if 0 <= room_count <= 5:  # –†–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
                        rooms.append(room_count)
            
            # –ü–æ–∏—Å–∫ –ø–ª–æ—â–∞–¥–µ–π
            area_elements = soup.find_all(text=re.compile(r'\d+(?:\.\d+)?\s*–º¬≤'))
            areas = []
            for area_text in area_elements[:10]:
                area_match = re.search(r'(\d+(?:\.\d+)?)\s*–º¬≤', str(area_text))
                if area_match:
                    area_val = float(area_match.group(1))
                    if 15 <= area_val <= 300:  # –†–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã –¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä
                        areas.append(area_val)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å–∏ –æ –∫–≤–∞—Ä—Ç–∏—Ä–∞—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            max_items = max(len(prices), len(rooms), len(areas), 1)
            
            for i in range(min(max_items, 10)):  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–≤–∞—Ä—Ç–∏—Ä —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                apartment = {
                    'complex_name': name,
                    'developer': developer,
                    'price': prices[i] if i < len(prices) else None,
                    'rooms': rooms[i] if i < len(rooms) else None,
                    'area': areas[i] if i < len(areas) else None,
                    'source_url': url,
                    'found_at': datetime.now().isoformat()
                }
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –∫–∞–∫–∞—è-—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if apartment['price'] or apartment['rooms'] or apartment['area']:
                    project_info['apartments'].append(apartment)
            
            return project_info if project_info['apartments'] else None
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –ø—Ä–æ–µ–∫—Ç–∞ {url}: {e}")
            return None
    
    def scrape_all_developers(self) -> Dict[str, List[Dict]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤
        """
        all_data = {}
        
        for dev_code, dev_info in self.developers.items():
            print(f"\n{'='*50}")
            print(f"–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞: {dev_info['name']}")
            print(f"{'='*50}")
            
            try:
                data = dev_info['parser']()
                all_data[dev_code] = data
                print(f"–ü–æ–ª—É—á–µ–Ω–æ {len(data)} –ø—Ä–æ–µ–∫—Ç–æ–≤ –æ—Ç {dev_info['name']}")
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {dev_info['name']}: {e}")
                all_data[dev_code] = []
            
            time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–∞–º–∏
        
        return all_data
    
    def save_to_json(self, data: Dict, filename: str = None):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª
        """
        if filename is None:
            filename = f"scraped_developers_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
            return filename
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞
    """
    print("üèóÔ∏è  –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä–∞")
    print("=" * 60)
    
    scraper = KrasnodarDeveloperScraper()
    
    # –ü–∞—Ä—Å–∏–º –≤—Å–µ—Ö –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤
    all_data = scraper.scrape_all_developers()
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n" + "=" * 60)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê")
    print("=" * 60)
    
    total_projects = 0
    total_apartments = 0
    
    for dev_code, projects in all_data.items():
        dev_name = scraper.developers[dev_code]['name']
        project_count = len(projects)
        apartment_count = sum(len(project.get('apartments', [])) for project in projects)
        
        print(f"{dev_name}:")
        print(f"  - –ü—Ä–æ–µ–∫—Ç–æ–≤: {project_count}")
        print(f"  - –ö–≤–∞—Ä—Ç–∏—Ä: {apartment_count}")
        print()
        
        total_projects += project_count
        total_apartments += apartment_count
    
    print(f"–ò–¢–û–ì–û:")
    print(f"  - –í—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–æ–≤: {total_projects}")
    print(f"  - –í—Å–µ–≥–æ –∫–≤–∞—Ä—Ç–∏—Ä: {total_apartments}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    filename = scraper.save_to_json(all_data)
    
    if filename:
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    return all_data

if __name__ == "__main__":
    main()